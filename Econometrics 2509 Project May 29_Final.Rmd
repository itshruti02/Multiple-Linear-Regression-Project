---
title: 'Econometrics 2509: Regression Group Project'
author: "Xinyu Li, Serhii Ovsianyk, Shruti Deshpande, and Denis Wu"
date: "May 29, 2020"
output:
  html_document: default
  word_document: default
  header-includes: \usepackage{color}
  pdf_document: default
margin: 1in
fontsize: 12pt
---
```{r setup, echo=FALSE, message=F, warning=F, include=FALSE}
# Clear the working space
rm(list = ls())

### Load the packages (all must have been installed)
library(ggplot2)
library(stargazer)
library(stringr)
library(lubridate)
library(car)
library(sandwich)
library(lsr)
library(ggpubr)

cse=function(reg) {
  rob=sqrt(diag(vcovHC(reg, type="HC1")))
  return(rob)
}

factor2numeric <- function(f){
   if(!is.factor(f)) stop("the input must be a factor")
   as.numeric(levels(f))[as.integer(f)]
}
```
*** 
# Abstract

This project aims to use the knowledge gained in Econometrics class to determine the causal effect of Type (free or paid) on the overall rating of an app on Google Play Store (https://www.kaggle.com/lava18/google-play-store-apps). We determine the effect via multiple linear regression. We observed that Type has an effect on Rating at 0.05 significance level and in the process of finding the best coefficient estimate for Type, we found that our final model showed difference in only hundredth decimal place, whereas our dependent variable Rating is measured only as precisely as the tenth decimal place. We believed that the true coefficient could be refined if we had additional variables in the dataset.
 
*** 
# Introduction

In recent years, the mobile software development community has exploded with mobile applications (hereafter referred to as apps) and has maintained an upward trajectory. This translates into a tremendous success for the app economy but additional competition for each developer. For any app developer, high ratings directly translate to success.

## Problem Statement

Android is expanding as an operating system. It has captured around 74% of the total market which indicates its high adoption rate. Our aim is to help android developers identify if app ratings are influenced based on the app type (free or paid).

## Goal

Our goal is to run a multiple linear regression that determines the causal effect of Type (paid or free) on the Rating of the applications on Google Play Store. The coefficients generated by our regression would help guide the developer before committing real resources to the task. It will also help to find out other factors that affect someone’s decision to rate an app.

## Hypotheses

### Null Hypothesis
Type (Paid or Free) has no effect on the Rating.

### Alternate Hypothesis
Type (Paid or Free) has an effect on Rating at 0.05 significance level.

***
# Data

## Control Variable Selection

To establish causal effect, we need necessary control variables. We followed a 3-step process to select valid control variables: 

* Brainstorm as a team, to identify potential variables based on our personal experience 
* Research online literature to further enrich the potential list of control variables
* Shortlist the variables available in our data set that match the above criteria

### Step 1: Brainstorm as a team

Based on our personal knowledge, we identified the following variables as potential control variables:

1.User experience: A simple and intuitive user experience intertwined with expected functionality could significantly impact app store rating (i.e., easy to learn and use).

2.Popularity: When a user is browsing the app-stores, they are naturally going to be more drawn to the apps that those around them and their friends use. In other words, rating would be affected by popularity. 

3.Age Groups: Certain user groups are more likely to rate higher or lower than others. For example, those who are older may be more lenient on their ratings and give higher ratings than those who are younger and be more demanding. 

4.Budget: A developer with greater resources would be more likely to develop higher quality apps, due to being able to hire more developers, hire more experienced developers, etc...Rating would highly be affected by this variable.

5.Category: Different app categories attract different users who may have different expectations. For example, those who use Beauty apps may tend to rate those apps highly because the apps make them feel good, whereas those who play game apps may rate those apps with lower scores because they feel frustration when trying to beat the game.

6.Company Reputation: A good brand develops trust with users that could translate to higher ratings.

### Step 2: Research Online Literature

Besides team brainstorming, we referred to the following literature links to confirm or add missing the potential list of control variables:

* https://www.pomsmeetings.org/confpapers/043/043-0354.pdf

* https://web.cs.ucdavis.edu/~filkov/papers/app_popularity.pdf

* https://www.ayetstudios.com/blog/app-store-optimization/app-store-ranking

After reviewing the literature, we added the following additional control variables to our potentials list: 

1.App Size: An app size may speak to an app’s number of features, which could have an effect on the rating.

2.Update Frequency: How frequently an update is updated speaks to how much a developer listens and cares about his target audience, which could have an impact on the rating.

### Step 3: Data Selection

Although we did not find a dataset that contained all of our ideal variables, we found the following dataset provided on publicly available website Kaggle: https://www.kaggle.com/lava18/google-play-store-apps. The variables contained in this dataset will help us estimate the causal effect on Type on Rating:

1.Category – Luckily, this dataset contains the variable category.

2.Installs – This categorical variable captures the number of installations for an app, which will do nicely as our variable to represent popularity.

3.Content.Rating- This categorical variable captures our variable for age groups, as it captures the different age groups the apps are appropriate for.

4.Reviews - The variable captures the number of reviews an app has received. This could also help us represent popularity somewhat.

5.Last.Updated - The variable captures the date on which the app is last updated. This variable can nicely represent the update frequency variable. 

6.Size - This variable refers to the size of the app, which is exactly one of the variables that our literature review suggests is important.

Unfortunately, this dataset is missing budget, user experience, and company reputation as control variables. We would just have to work with what we have. The dataset also contains some other variables that we don't believe are meaningful and will be dropped as seen below.

### Import the Dataset
```{r data, echo=FALSE, warning=FALSE, include=FALSE}
data <- read.csv('googleplaystore.csv')
```

```{r, echo=FALSE}
print(paste('Number of rows:',nrow(data)))
summary(data)
```

Check the first couple of rows
```{r}
head(data)
```

In order to better understand our dataset, next we perform Exploratory Data Analysis and Data Cleaning.

***
# Exploratory Data Analysis and Data Cleaning

By sifting through our dataset, there are 10841 rows and 13 columns. Before we deep dive into the data analysis process, we have to perform some data cleaning on some columns.

### Clean Rating column
```{r}
#Get a sense of our Rating column
head(unique(data$Rating))
```


```{r, echo=FALSE}
print(paste(sum(is.na(data$Rating))*100/nrow(data), '% of Rating is NA'))
# Unfortunately, a rather high percentage of our records contain NA for rating
```

```{r}
data <- data[!is.na(data$Rating),]

max(data$Rating, na.rm = TRUE)
# max is showing as 19, but we know that the rating range only goes from 1 to 5, so we drop this row
data <- data[data$Rating != 19,]
```

We will remove outliers, using 1.5*IQR rule.

$Outlier < Q_1 - 1.5*IQR$

$Outlier > Q_3 + 1.5*IQR$

```{r}
rating_3rd_q = quantile(data$Rating, 0.75)[[1]]
rating_1st_q = quantile(data$Rating, 0.25)[[1]]
ratingIQR = rating_3rd_q - rating_1st_q

data <- data[data$Rating < rating_3rd_q + 1.5 * ratingIQR,]
data <- data[data$Rating > rating_1st_q - 1.5 * ratingIQR,]
```


```{r}
# Shows counts of each unique values after cleaning
table(data$Rating)
```

### Clean Type column
```{r}
table(data$Type)
```

We see that we have two levels that are not used so let's remove them:
```{r}
data$Type <- data$Type[ , drop=TRUE]
table(data$Type)
```

### Clean Reviews columns
```{r}
data$Reviews <- factor2numeric(data$Reviews)
# We verify that there are no NaN values after the conversion
head(data[data$Reviews == 'NaN',])
```

Once again, we will remove outliers, using 1.5*IQR rule.
```{r}
Reviews_3rd_q = quantile(data$Reviews, 0.75)[[1]]
Reviews_1st_q = quantile(data$Reviews, 0.25)[[1]]
ReviewsIQR = Reviews_3rd_q - Reviews_1st_q

data <- data[data$Reviews < Reviews_3rd_q + 1.5 * ReviewsIQR,]
data <- data[data$Reviews > Reviews_1st_q - 1.5 * ReviewsIQR,]
nrow(data)
```


### Clean Size column
```{r}
# We notice that these values are stored as factors and contain different units
tail((table(data$Size)))
```

```{r}
# Some records contain 'Varies with device' as the value for Size, which we consider to be useless data.
head(data[data$Size == 'Varies with device',])
```

We remove these useless records from our dataset:
```{r}
data <- data[data$Size != 'Varies with device',]
```

Since the values are factors, we convert them into characters:
```{r}
data$Size <- as.character(data$Size)
```

We create functions to help convert all units to be kilobytes:
```{r}
substrRight <- function(x, n){
  substr(x, nchar(x), nchar(x))
}

substrLeft <- function(x, n){
  substr(x, 0, nchar(x)-1)
}

convertSizeToKilo <- function(size)
{
  converted <- 0
  if (substrRight(size) == 'M') {
    converted <- as.numeric(substrLeft(size)) * 1000
  }
  else {
    converted <- as.numeric(substrLeft(size))
  }
  return(converted)
}

```

Apply conversion
```{r}
 data$Size <- sapply(data$Size,convertSizeToKilo)
```

Verify Size to not contain NaN values:
```{r}
head(data[data$Size == 'NaN',])
```

We will remove outliers, using 1.5*IQR rule.
```{r}
size_3rd_q = quantile(data$Size, 0.75)[[1]]
size_1st_q = quantile(data$Size, 0.25)[[1]]
sizeIQR = size_3rd_q - size_1st_q

data <- data[data$Size < size_3rd_q + 1.5 * sizeIQR,]
data <- data[data$Size > size_1st_q - 1.5 * sizeIQR,]
```

### Price Column

Since price creates multicollinearity issues with our variable of interest Type, we will drop this column from our analysis (Price is just a more informative version of Type).
```{r, echo=FALSE}
# it might be not important for our first research question. But later we can investigate how does price affect Rating.
#data$Price <- as.character(data$Price)

#df_n0 <- data[data$Price != "0",]
#df_0 <- data[data$Price == "0",]

#for (row in 1:nrow(df_n0)){
#  if(substr(df_n0[row,"Price"],1,1) == "$"){
#    df_n0[row,"Price"] <- substr(df_n0[row,"Price"],2,str_length(df_n0[row,"Price"]))
#  }
#}

#data <- rbind(df_n0, df_0)

#data$Price <- as.numeric(data$Price)

# good
#nrow(data[(data$Price != 0) & (data$Type == 'Free'),])
```

### Cleaning Content.Rating column
```{r}
# data$Content.Rating == '' rows were filltered out before
unique(data$Content.Rating)

# 1 row, seems reasonable to drop it
data <- data[data$Content.Rating != 'Unrated',]
```

### Transforming Last.Updated Column

To make use of the the Last.Updated column, we can convert its date values to number of days since 2019-01-01:
```{r}
data$Last.Updated <- mdy(data$Last.Updated)
data$datainterval <- time_length(interval(data$Last.Updated,ymd("2019-01-01")),'day')
```

We will remove outliers, using 1.5*IQR rule.
```{r}
datainterval_3rd_q = quantile(data$datainterval, 0.75)[[1]]
datainterval_1st_q = quantile(data$datainterval, 0.25)[[1]]
dataintervalIQR = datainterval_3rd_q - datainterval_1st_q

data <- data[data$datainterval < datainterval_3rd_q + 1.5 * dataintervalIQR,]
data <- data[data$datainterval > datainterval_1st_q - 1.5 * dataintervalIQR,]
nrow(data)
# 5877
```

### Current.Ver Column

This column is not useful since the versions are not standardized across apps.
```{r}
head(unique(data$Current.Ver))
```

### Android.Ver Column

We see from the different unique values that this column is not useful either, since many of them overlap.
```{r}
table(data$Android.Ver)
```

### Genres Column

Since the Genres column is actually a subset of Categories, and there are far too many different genres, we will stick to only using Categories.
```{r, echo=FALSE}
# Keep this stuff to show that we tried

#unique(data$Genres)
#data$Genres <- as.character(data$Genres)

#for (row in 1:nrow(data)){
#  if(grepl(';', data[row, "Genres"], fixed = TRUE)){
#    data[row, "Genres"] <- strsplit(data[row, "Genres"], ';')[[1]][1]
#  }
#}


#
#
# unique(data$Genres)
# data$Genres <- as.character(data$Genres)
# 
# for (row in 1:nrow(data)){
#   if(grepl(';', data[row, "Genres"], fixed = TRUE)){
#     data[row, "Genres"] <- strsplit(data[row, "Genres"], ';')[[1]][1]
#   }
#   
#   if(data[row, "Genres"] == "Music"){
#     data[row, "Genres"] <- "Music & Audio"
#   } else if(is.element(data[row, "Genres"],c("Board","Puzzle","Strategy","Simulation","Action","Role Playing","Casual","Card","Adventure","Racing","Casino","Trivia"))){
#     data[row, "Genres"] <- "Game"
#   } else if(is.element(data[row, "Genres"],c("Books & Reference","Word"))){
#     data[row, "Genres"] <- "Game"
#   }
#   
# 
# "Books & Reference""Word"
# "Social""Communication""News & Magazines"
# "Finance""Business"
# "Art & Design","House & Home"-> "Lifestyle"
# "Health & Fitness""Sports"
# "Education"<- "Educational"
#   
#   
# }

#unique(data$Genres)
```

### Transforming Category Column

We reduced the number of different categories, as we had 34, which was a lot. We grouped the most closely associated categories into a larger category. For example, if a category was “Finance,” we choose to place it under “Business.”  Implementing this approach helped us reduce to 23 factors, or 32% less.
```{r}
unique(data$Category)

data$Category <- as.character(data$Category)

for (row in 1:nrow(data)){
  if(data[row, "Category"] == "FINANCE"){
    data[row, "Category"] <- "BUSINESS"
  } else if(data[row, "Category"] == "SPORTS"){
    data[row, "Category"] <- "HEALTH_AND_FITNESS"
  } else if(is.element(data[row, "Category"],c("COMMUNICATION", "EVENTS"))){
    data[row, "Category"] <- "SOCIAL"
  } else if(data[row, "Category"] == "PARENTING"){
    data[row, "Category"] <- "FAMILY"
  } else if(data[row, "Category"] == "PHOTOGRAPHY"){
    data[row, "Category"] <- "ART_AND_DESIGN"
  } else if(is.element(data[row, "Category"],c("COMICS", "BOOKS_AND_REFERENCE", "NEWS_AND_MAGAZINES", "LIBRARIES_AND_DEMO"))){
    data[row, "Category"] <- "LITERATURE"
  } else if(data[row, "Category"] == "MAPS_AND_NAVIGATION"){
    data[row, "Category"] <- "TRAVEL_AND_LOCAL"
  }
}

data$Category <- as.factor(data$Category)

unique(data$Category)

```

***

# Analysis

## Overview of Data

Category
```{r, echo=FALSE}
table(data$Category)
```

Content.Rating
```{r, echo=FALSE}
table(data$Content.Rating)
```

Installs
```{r, echo=FALSE}
table(data$Installs)
```

```{r, echo=FALSE}
stargazer(data[c("Rating", "Reviews", "Size", "datainterval")], type="text", digits=2, summary.stat=c("n", "mean", "sd"), title="Google Playstore Apps", flip=FALSE,
          covariate.labels=c("Rating","Reviews", "Size", "Days Since Last Update"))
```


## Validation of Control Variables

We will use statistical tools to check for correlations and statistical significance between our variable of interest and potential control variables, as well as for correlations between our dependent variable and potential control variables.

### Categorical Variables

We use Chi-squared test and Cramer's V, for the case when both variables are categorical.

#### Validation for `Category`
```{r}
# relation with variable of interest
chisq.test(data$Type, data$Category)
cramersV(data$Type, data$Category)
```

After Chi-squared test and Cramer's V with variable of interest, we can see the result of P-Value is less than 0.05 and that there's a positive correlation.

Correlation between `Rating` and `Category`
```{r}
# relation with dependent variable
summary(aov(Rating ~ Category, data = data))
```

We also observe that there's a relationship between `Rating` and `Category`, so the `Category` variable should be valid.

#### Validation for `Install`
```{r}
# relation with variable of interest
chisq.test(data$Type, data$Installs)
cramersV(data$Type, data$Installs)
```

After Chi-squared test and Cramer's V with variable of interest, we can see the result of P-Value is less than 0.05 and that there's a positive correlation.

Correlation between `Rating` and `Installs`
```{r}
# relation with dependent variable
summary(aov(Rating ~ Installs, data = data))
```

We also observe that there's a relationship between `Rating` and `Installs`, so the `Installs` variable should be valid.

#### Validation for `Content.Rating`
```{r}
# relation with variable of interest
chisq.test(data$Type, data$Content.Rating)
cramersV(data$Type, data$Content.Rating)
```

After Chi-squared test and Cramer's V with variable of interest, we can see the result of P-Value is less than 0.05, and that there's a weakly positive correlation.

Correlation between `Rating` and `Content.Rating`
```{r}
# relation with dependent variable
summary(aov(Rating ~ Content.Rating, data = data))
```

We also observe that there's a relationship between `Rating` and `Content.Rating`, so the `Content.Rating` variable should still be valid.

### Continuous Variable

We use unpaired t-test, for the case when one variable is categorical and the other is continuous.

#### Validation for `Reviews`
```{r}
# relation with variable of interest
group1 = data[data$Type == 'Free','Reviews']
group2 = data[data$Type == 'Paid','Reviews']
t.test(group1, group2, alternative = "two.sided", var.equal = FALSE)
```

After t.test with variable of interest, we can see the result of P-Value is less than 0.05, 

Correlation between `Rating` and `Reviews`
```{r}
# relation with dependent variable
cor(data$Reviews, data$Rating)
```

We observe that although the relationship between `Rating` and `Content.Rating` is weak, the `Reviews` variable should still be valid.

#### Validation for `datainterval`
```{r}
# relation with variable of interest
group1 = data[data$Type == 'Free','datainterval']
group2 = data[data$Type == 'Paid','datainterval']
t.test(group1, group2, alternative = "two.sided", var.equal = FALSE)
```

After t.test with variable of interest, we can see the result of P-Value is less than 0.05.

Correlation between `Rating` and `datainterval`
```{r}
# relation with dependent variable
cor(data$datainterval, data$Rating)
```

We observe a negative relationship between `Rating` and `Content.Rating` is weak, so the `datainterval` variable should be valid.

#### Validation for `Size`
```{r}
# relation with variable of interest
group1 = data[data$Type == 'Free','Size']
group2 = data[data$Type == 'Paid','Size']
t.test(group1, group2, alternative = "two.sided", var.equal = FALSE)
```

After t.test with variable of interest, we can see the result of P-Value is more than 0.05.

Correlation between `Rating` and `Size`
```{r}
# relation with dependent variable
cor(data$Size, data$Rating)
```

Since the `Size` variable is not statistical significant and its relationship to `Rating` is weak, it should not be a valid control variable.

### Conclusions for Control Variable Validation

Below is a summary of the above tests:

![](test_table.png)

Based on our statistical tests and correlations, we decide that all of our potential control variables are valid, except for `Size`.

#### Valid Control Variables:

1. Categorical Variables: `Category`, `Installs`, `Content.Rating`

2. Continuous Variable: `Reviews`, `datainterval`

## Graphs of Important Variables

### Graph 1: `Rating` Distribution
```{r}
plt <- ggplot(data, aes(x = Rating)) + labs(title = "Rating distribution", x = "Rating", y = "Relative frequency")
plt + geom_histogram(aes(y = ..density..), colour="black", fill="white") + geom_density(alpha = 0.2, fill = "#FF6666")
```

We see that most apps rate between 4 and 5. Addtitionaly, this distribution tells us that an improvement of 0.1 in `Rating` in the range of 4 and 5 is actually fairly important to Google PlayStore apps. There's a large hump to pass beyond a rating of 4.4 to stand apart from the rest.

### Graph 2: `Rating` ~ `Reviews`
```{r}
ggplot(data, aes(x = Reviews, y=Rating)) + geom_point() +
stat_smooth(method = "lm", col = "red", se=FALSE)
```

We have chosen to plot and understand the correlation between `Rating` and `Review` because:

* the number of reviews infers the popularity of the app

* this correlation essentially helps us explain the association between an app’s rating and its popularity

Additionally, we can see that ratings of applications are positively influenced by the number of reviews. In other words, ratings increase with an increase in number of reviews, although very weakly. Most importantly, we notice that `Reviews` seems to be slightly logarithmic, meaning we should consider transforming `Reviews` with log().

### `Rating` ~ `datainterval`

We also selected to look at the number of days from `Last.Update` (graph is not presented due to strict rubric limitation), as it can be an indication of how much the developers care about the app. Typically, if a developer cares more, the number of days since last update would be smaller. The graph didn't look particularly polynomial or logarithmic.

## Model Specification

We first check each valid control variable for omitted variable bias.

`fit1` - Only has `Type`

`fit2` - `Type` + `Category`

`fit3` - `Type` + `Installs`

`fit4` -  `Type` + `Reviews`

`fit5` -  `Type` + `datainterval`

`fit6` -  `Type` + `Content.Rating`

```{r}
fit1 <- lm(Rating ~ Type, data=data)

fit2 <- lm(Rating ~ Type + Category, data=data)

fit3 <- lm(Rating ~ Type + Installs, data=data)

fit4 <- lm(Rating ~ Type + Reviews, data=data)

fit5 <- lm(Rating ~ Type + datainterval, data=data)

fit6 <- lm(Rating ~ Type + Content.Rating, data=data)

stargazer(fit1, fit2, fit3, fit4, fit5, fit6,se=list(cse(fit1), cse(fit2), cse(fit3), cse(fit4), cse(fit5), cse(fit6)),title="Omitted Variable Bias", type="text", df=FALSE, digits=3, column.labels = c("fit1", "fit2","fit3","fit4","fit5","fit6"), single.row = TRUE,column.sep.width = "1pt")
```

We see that all of them have some effect, so we keep all of them and generate our base model specification as `fit7` which has all the valid control variables.

## Base and Alternative Model Specifications

### Base Model Specifications
`fit7` - Base model specification which has all the valid control variables. The coefficient of `Type` is actually statistically insignificant, so we'll need to look for alternatives.

### Alternative Model Specifications 1
`fit8` - Because we observed earlier that `Reviews` seems to be slightly logarithmic, we transform `Reviews` to `log(Reviews)`. Below, we can see that it had an impact on the coeficient of the variable of interest, and it is now statistically significant.

### Alternative Model Specifications 2
`fit9` - We believe that given the `Type`, the number of `Reviews` would have different impact on `Rating`, so we build the interaction term `Type*log(Reviews)` to check if it has significant impact on a coefficient of the variable of interest. This actually had a dramatic impact on a coefficient of the variable of interest which brings the coefficient from -0.08 to -0.25.

### Alternative Model Specifications 3
`fit10` - Although `Size` is not statistically significant from our findings earlier, we still wanted to check if more days without updating app before 2019-01-01 in relation to the `Size` of the app would have an impact on `Rating`, so we build the interaction term `Size*datainterval`. This had a small impact on a coefficient of the variable of interest which brings the coefficient from -0.25 to -0.24. This suggests that we are starting to converge.

### Alternative Model Specifications 4
`fit11` -  We also wonder whether for more number of `Install`, the influence of `Size` on `Rating` will change or not, so we build the interaction term `Size*Installs` to check if it has significant impact on a coefficient of the variable of interest. 
This also had only a small impact on a coefficient of the variable of interest which brings the coefficient from -0.24 to -0.25.

### Alternative Model Specifications 5
`fit12` - As our regression terms have reach the limit of 10 in `fit11`, we will not show any further models with more interaction terms. However, our own tests suggests that the coefficient for `Type` has converged to -0.25.


```{r}
fit7 <- lm(Rating ~ Type + Category + Installs + Reviews + datainterval + Content.Rating, data=data)

fit8 <- lm(Rating ~ Type + Category + Installs + log(Reviews) + datainterval + Content.Rating, data=data)

fit9 <- lm(Rating ~ Type*log(Reviews) + Category + Installs + datainterval + Content.Rating, data=data)

fit10 <- lm(Rating ~ Type*log(Reviews) + Category + Installs + Content.Rating + Size*datainterval, data=data)

fit11 <- lm(Rating ~ Type*log(Reviews) + Category + Content.Rating + Size*datainterval + Size*Installs, data=data)

stargazer(fit7, fit8, fit9, fit10, fit11,se=list(cse(fit7), cse(fit8), cse(fit9), cse(fit10),cse(fit11)),title="Base Model and Alternative Specifications", type="text", df=FALSE, digits=3, column.labels = c("fit7", "fit8","fit9","fit10","fit11"),column.sep.width = "1pt")
```


## Check Restricted vs Unrestricted Models

### fit7 vs fit9

```{r echo=FALSE, warning=FALSE}
anova(fit7,fit9)
```

Since our p-value < 0.05, we should reject the restricted/base model (Model 7) and keep looking for alternative models.

### fit9 vs fit10

```{r echo=FALSE, warning=FALSE}
anova(fit9,fit10, test='Chisq')
```

Since our p-value < 0.05, we should reject the restricted/base model (Model 9) and keep looking for alternative models.

### fit10 vs fit11

```{r echo=FALSE, warning=FALSE}
anova(fit10,fit11, test='Chisq')
```

Since our p-value > 0.05, we fail to reject the restricted/base model. We should keep this restricted/base (Model10).

# Conclusion

## Final Model

We actually decide that our final model should be `fit9`, even though our statistical tests suggests that `fit10` is statistically significant when the two models are compared.

```{r}
summary(fit9)
```
                                                       
We can see from the stargazer table earlier that the coefficients of `Type`, which is our variable of interest, converge around -0.25, with some small difference between each other in the hundredth decimal place. Because our dependent variable `Rating` is measured only as precisely as the tenth decimal place (i.e. 3.5, 3.6, 4.1, 4.6, etc...), we believe that the more statistically significant models are not more economically significant than `fit9`. As such, we would elect to use a less complicated model with fewer confounding interaction terms -`fit9`.

## Model Hightlights

1.Holding everything else constant, if it's a paid type, the rating will decrease by 0.25 points.

2.Holding everything else constant, an increase of reviews by 1% will increase the rating by 0.0010.

3.Each category will have different impact on rating:

  * If the app is a dating app, the rating will decrease by 0.185 points.
  
  * If the app is a education app, the rating will increase by 0.113 points.
  
  * If the app is a tool app, the rating will decrease by 0.12 points.

4.Holding everything else constant, an increase of date interval by 1 day will decrease the rating by 0.00021, which means apps with more recent updates will typically have higher rating.

5.Interaction term: For more reviews, when reviews increase 1%，the influence of type on rating will increase，which means if the app has more and more reviews, the negative impact of paid app on rating will decrease.

6.Goodness of fit statistics:
As our models became more complicated, the adjusted R-squared increased, which meant that the goodness of fit of our models were increasing. 
In our final model, our adjusted R-squared was 0.203, which meant our model captured around 20% information.

## Suggestions for Developers:

1.In general, a free app is more likely to have a higher `Rating`.

2.Updating the apps will help generate higher `Rating` as well.

3.A larger app file `Size` is an indication of having more features. However, having too many features (larger file `Size`) could bring down an app's `Rating`.

## Internal Validity

After we corrected for Heteroskedasticity and removed outliers and Multicollinearity variables, we believe our estimator for our variable of interest to be unbiased, but only for this dataset. We have other important control variables that are missing from our dataset, such as budget and company reputation, and user experience. Our results would have been more robust had we had a dataset that contains those control variables.

## External Validity

We only have data from before 2018, so our analysis may not apply to the more recent apps in 2019 and 2020.

## Causal Effect

Based on our results, we can reject the Null Hypothesis and say that `Type` (Paid or Free) of the applications has an effect on Rating at 0.05 significance level.

Additionally, because our dependent variable `Rating` is measured only as precisely in the tenth decimal place (i.e. 3.5, 3.6, 4.1, 4.6, etc...), we believe that the more statistically significant models are not more economically significant than `fit9`. As such, our estimate for Type is -0.25.

Given our limited dataset, we have reached the best coefficient for our variable of interest. However, we don't believe we have the actual true coefficient estimation for `Type` as we are missing important control variables such as budget, user experience, company reputation. A major lesson for this project is that we must focus on a topic to understand first, before select a dataset to make a study with.
